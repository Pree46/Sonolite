{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10925989,"sourceType":"datasetVersion","datasetId":6792841},{"sourceId":278340,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":238416,"modelId":260084}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super().__init__()\n        self.add_relu = torch.nn.quantized.FloatFunctional()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        \n        \n        out = self.add_relu.add_relu(out, identity)\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=5):\n        super().__init__()\n\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        \n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n        \n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes, 1, stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n\n        self.inplanes = planes\n\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n\n        x = self.quant(x)\n        x = self.conv1(x) \n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x) \n\n        x = self.layer1(x)  \n        x = self.layer2(x) \n        x = self.layer3(x) \n        x = self.layer4(x)\n\n        \n        x = self.dequant(x)\n        return x\n\n\ndef resnet34():\n    print('RESNET34 BACKBONE')\n    layers=[3, 4, 6, 3]\n    \n    model = ResNet(BasicBlock, layers)\n    return model\n\n\ndef get_object_detection_model(num_classes=5):\n\n    backbone = resnet34()\n\n    \n    backbone.out_channels = 512\n    \n    anchor_generator = AnchorGenerator(sizes=((128, 256, 512),),\n                                       aspect_ratios=((0.5, 1.0, 2.0),))\n\n    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator)\n    return model\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = get_object_detection_model(num_classes=5)\nmodel.load_state_dict(torch.load('/kaggle/input/fpus23/saved_models/OD_34/quant_model.pth', map_location=device))\nmodel.to(device)\nmodel.eval()\n\nprint(\"Model loaded successfully on: \", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:03:39.281193Z","iopub.execute_input":"2025-03-07T15:03:39.281574Z","iopub.status.idle":"2025-03-07T15:03:42.188077Z","shell.execute_reply.started":"2025-03-07T15:03:39.281541Z","shell.execute_reply":"2025-03-07T15:03:42.186220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torchvision.transforms as T\n\ntransform = T.Compose([\n    T.ToTensor()\n])\n\nimage = Image.open('/kaggle/input/fpus23/Dataset_Plane/FL_PLANE/0zXampIL.png').convert(\"RGB\")\nimage = transform(image).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:04:01.313760Z","iopub.execute_input":"2025-03-07T15:04:01.314154Z","iopub.status.idle":"2025-03-07T15:04:01.391285Z","shell.execute_reply.started":"2025-03-07T15:04:01.314125Z","shell.execute_reply":"2025-03-07T15:04:01.390030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n    outputs = model([image])\n\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:04:13.789548Z","iopub.execute_input":"2025-03-07T15:04:13.789952Z","iopub.status.idle":"2025-03-07T15:04:16.801152Z","shell.execute_reply.started":"2025-03-07T15:04:13.789924Z","shell.execute_reply":"2025-03-07T15:04:16.799896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.7\npred_boxes = outputs[0]['boxes'].cpu().numpy()\npred_scores = outputs[0]['scores'].cpu().numpy()\npred_labels = outputs[0]['labels'].cpu().numpy()\n\nfor box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n    if score > threshold:\n        print(f\"Detected class {label} with confidence {score} at box {box}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:06:14.507395Z","iopub.execute_input":"2025-03-07T15:06:14.507899Z","iopub.status.idle":"2025-03-07T15:06:14.516841Z","shell.execute_reply.started":"2025-03-07T15:06:14.507841Z","shell.execute_reply":"2025-03-07T15:06:14.515015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\nCLASSES = {1: \"Head\", 2: \"Abdomen\", 3: \"Arms\", 4: \"Legs\"}\nCLASS_COLORS = {\n    1: 'red',      \n    2: 'blue',      \n    3: 'green',    \n    4: 'orange'    \n}\n\nfig, ax = plt.subplots(1)\nax.imshow(Image.open('/kaggle/input/fpus23/Dataset_Plane/FL_PLANE/0zXampIL.png'))\n\nfor box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n    if score > threshold:\n        color = CLASS_COLORS.get(label, 'white') \n        rect = patches.Rectangle(\n            (box[0], box[1]),\n            box[2] - box[0],\n            box[3] - box[1],\n            linewidth=2,\n            edgecolor=color,\n            facecolor='none'\n        )\n        ax.add_patch(rect)\n        ax.text(\n            box[0], box[1],\n            f'{CLASSES[label]}: {score:.2f}',\n            color='white',\n            bbox=dict(facecolor=color, alpha=0.5)\n        )\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:06:20.527674Z","iopub.execute_input":"2025-03-07T15:06:20.528074Z","iopub.status.idle":"2025-03-07T15:06:20.789805Z","shell.execute_reply.started":"2025-03-07T15:06:20.528044Z","shell.execute_reply":"2025-03-07T15:06:20.788391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/pytorch/vision.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:28:12.136291Z","iopub.execute_input":"2025-03-09T06:28:12.136632Z","iopub.status.idle":"2025-03-09T06:28:12.264369Z","shell.execute_reply.started":"2025-03-09T06:28:12.136600Z","shell.execute_reply":"2025-03-09T06:28:12.263496Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'vision' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/working/vision/references/detection\")\n\nfrom engine import train_one_epoch, evaluate\nimport utils \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:28:14.897391Z","iopub.execute_input":"2025-03-09T06:28:14.897741Z","iopub.status.idle":"2025-03-09T06:28:14.935063Z","shell.execute_reply.started":"2025-03-09T06:28:14.897711Z","shell.execute_reply":"2025-03-09T06:28:14.934437Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"%%writefile OD_US_Dataset.py\n\nimport os\nimport numpy as np\nimport cv2\nimport torch\nfrom xml.etree import ElementTree as ET\nfrom torchvision import transforms as torchtrans\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\nclass OD_US_Dataset(torch.utils.data.Dataset):\n    def __init__(self, ano_path, img_path, transforms=None):\n        self.classes = ['bkg', 'Head', 'Abdomen', 'Arms', 'Legs']\n        self.ano_path = ano_path\n        self.img_path = img_path\n        self.transforms = transforms\n        self.X = []\n        self.Y = []\n        self.LA = []\n\n        for obj in os.listdir(ano_path):\n            file_name = os.path.join(ano_path, obj, 'annotations.xml')\n            dom = ET.parse(file_name)\n            names = dom.findall('image')\n\n            for n in names:\n                bbox = []\n                la = []\n                name = n.attrib.get('name')\n                lab = n.findall('box')\n\n                if lab:\n                    for l in lab:\n                        xtl, ytl, xbr, ybr = float(l.attrib.get('xtl')), float(l.attrib.get('ytl')), float(l.attrib.get('xbr')), float(l.attrib.get('ybr'))\n                        label = l.attrib.get('label').lower()\n\n                        label_map = {'bkg': 0, 'head': 1, 'abdomen': 2, 'arm': 3, 'legs': 4}\n                        if label in label_map:\n                            la.append(label_map[label])\n\n                        bbox.append([xtl, ytl, xbr, ybr])\n\n                    img_path_full = os.path.join(img_path, obj, name)\n                    self.Y.append(bbox)\n                    self.X.append(img_path_full)\n                    self.LA.append(la)\n\n    def __getitem__(self, idx):\n        img_name = self.X[idx]\n        img = cv2.imread(img_name)\n        img_res = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n\n        labels = self.LA[idx]\n        boxes = np.array(self.Y[idx]).astype(float)\n\n        if len(boxes) == 0: \n            boxes = np.array([[0, 0, 1, 1]], dtype=np.float32)\n            labels = [0] \n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        image_id = torch.tensor([idx])\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"area\": area,\n            \"iscrowd\": iscrowd,\n            \"image_id\": image_id\n        }\n\n        if self.transforms:\n            sample = self.transforms(image=img_res,\n                                     bboxes=boxes.tolist(),\n                                     labels=labels.tolist())\n\n            img_res = sample['image']\n            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n\n        return img_res, target\n\n    def __len__(self):\n        return len(self.X)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:28:21.429272Z","iopub.execute_input":"2025-03-09T06:28:21.429579Z","iopub.status.idle":"2025-03-09T06:28:21.435305Z","shell.execute_reply.started":"2025-03-09T06:28:21.429554Z","shell.execute_reply":"2025-03-09T06:28:21.434594Z"}},"outputs":[{"name":"stdout","text":"Overwriting OD_US_Dataset.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/working/\")\nfrom OD_US_Dataset import OD_US_Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:28:26.598650Z","iopub.execute_input":"2025-03-09T06:28:26.598965Z","iopub.status.idle":"2025-03-09T06:28:26.602466Z","shell.execute_reply.started":"2025-03-09T06:28:26.598942Z","shell.execute_reply":"2025-03-09T06:28:26.601627Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"pip install -U albumentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:28:03.356228Z","iopub.execute_input":"2025-03-09T06:28:03.356665Z","iopub.status.idle":"2025-03-09T06:28:09.181728Z","shell.execute_reply.started":"2025-03-09T06:28:03.356638Z","shell.execute_reply":"2025-03-09T06:28:09.180641Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\nCollecting albumentations\n  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\nCollecting albucore==0.0.23 (from albumentations)\n  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\nCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nDownloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\nDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n  Attempting uninstall: albucore\n    Found existing installation: albucore 0.0.19\n    Uninstalling albucore-0.0.19:\n      Successfully uninstalled albucore-0.0.19\n  Attempting uninstall: albumentations\n    Found existing installation: albumentations 1.4.20\n    Uninstalling albumentations-1.4.20:\n      Successfully uninstalled albumentations-1.4.20\nSuccessfully installed albucore-0.0.23 albumentations-2.0.5 simsimd-6.2.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils.prune as prune\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision\nfrom torchvision import transforms as torchtrans\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\nfrom OD_US_Dataset import OD_US_Dataset\nwarnings.filterwarnings('ignore')\n\nimage_path = '/kaggle/input/fpus23/Dataset/four_poses/'\nannotation_path = '/kaggle/input/fpus23/Dataset/boxes/annotation/'\n\ntorch.backends.cudnn.benchmark = True  \n\ndef get_transform(train):\n    return A.Compose([\n        A.Resize(512, 512),\n        ToTensorV2(p=1.0)\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n\ndef collate_fn(batch):\n    batch = [b for b in batch if b is not None]\n    return tuple(zip(*batch)) if batch else None\n\ndataset = OD_US_Dataset(annotation_path, image_path, transforms=get_transform(train=True))\nprint(f\"Dataset Length: {len(dataset)}\\n\")\n\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ntest_split = 0.2\ntsize = int(len(dataset) * test_split)\n\ndataset_train = Subset(dataset, indices[:-tsize])\ndataset_test = Subset(dataset, indices[-tsize:])\n\ndata_loader = DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=8, pin_memory=True, collate_fn=collate_fn)\ndata_loader_test = DataLoader(dataset_test, batch_size=8, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate_fn)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f\"Using Device: {device}\")\n\ndef get_object_detection_model(num_classes=5):\n    backbone = resnet_fpn_backbone('resnet34', weights=\"DEFAULT\")\n    model = FasterRCNN(backbone, num_classes=num_classes)\n    return model\n\nnum_classes = 5\nmodel = get_object_detection_model(num_classes).to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n\nscaler = torch.cuda.amp.GradScaler()\n\nsave_path = '../saved_models/OD_34/'\nos.makedirs(save_path, exist_ok=True)\n\nnum_epochs = 10 \nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs} - Training Started...\")\n\n    model.train()\n    for images, targets in data_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast(): \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n        scaler.scale(losses).backward() \n        scaler.step(optimizer)\n        scaler.update()\n\n    lr_scheduler.step() \n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Evaluating...\")\n    evaluate(model, data_loader_test, device=device)\n\ntorch.save({\n    'epoch': num_epochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict()\n}, os.path.join(save_path, \"OD_34_ckpt_final.pth\"))\n\nprint(\"Training Completed Successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:30:48.995189Z","iopub.execute_input":"2025-03-09T06:30:48.995542Z"}},"outputs":[{"name":"stdout","text":"Dataset Length: 9455\n\nUsing Device: cuda\nEpoch 1/10 - Training Started...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}